import streamlit as st
import requests

# Tu token de Hugging Face (no compartir pÃºblicamente en producciÃ³n)
HF_TOKEN = "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"

st.set_page_config(page_title="Generador de ArtÃ­culos SEO", page_icon="ğŸ§ ")
st.title("ğŸ§  Generador de ArtÃ­culos con Hugging Face")

keyword = st.text_input("ğŸ”‘ Palabra clave principal", placeholder="Ej: inteligencia artificial")
style = st.selectbox("âœï¸ Estilo del artÃ­culo", ["Informativo", "Persuasivo", "Tutorial"])
length = st.slider("ğŸ“ Longitud del artÃ­culo (palabras)", 100, 1000, 300)

if st.button("ğŸš€ Generar artÃ­culo"):
    with st.spinner("Generando artÃ­culo..."):
        headers = {
            "Authorization": f"Bearer {HF_TOKEN}"
        }

        prompt = (
            f"Eres un redactor SEO experto. Escribe un artÃ­culo de {length} palabras, estilo {style}, "
            f"usando la palabra clave principal: '{keyword}'. Usa subtÃ­tulos y lenguaje claro."
        )

        payload = {
            "inputs": prompt
        }

        # Usamos modelo pÃºblico que no requiere permisos especiales
        url = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"

        response = requests.post(url, headers=headers, json=payload)

        if response.status_code == 200:
            result = response.json()
            # Algunos modelos devuelven lista con "generated_text"
            if isinstance(result, list) and "generated_text" in result[0]:
                st.subheader("ğŸ“„ ArtÃ­culo generado:")
                st.write(result[0]["generated_text"])
            else:
                st.error("El formato de respuesta no es el esperado.")
        else:
            st.error(f"Error {response.status_code}: No se pudo generar el artÃ­culo.")
